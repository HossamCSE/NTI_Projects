{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52245e9d-e670-496e-bcfc-d26731a18a09",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a7ac4a-282b-4652-9ee0-4db927aea04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                                        # used to handle files using system commands.\n",
    "import pickle                                                                    # used to store numpy features extracted.\n",
    "import numpy as np                                                               # used to perform a wide variety of mathematical operations on arrays.\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm                                                   # progress bar decorator for iterators. Includes a default range iterator printing to stderr.\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input          # imported modules for feature extraction from the image data.\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array          # used for loading the image and converting the image to a numpy array.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer                        # used for loading the text as convert them into a token.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences                # used for equal distribution of words in sentences filling the remaining spaces with zeros.\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model                    # used to visualize the architecture of the model through different images.\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import gc\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b515a4c-fb06-4329-bf75-7d127d371ee8",
   "metadata": {},
   "source": [
    "## Directories of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e22b584-aef6-4015-9c18-f0b624cf193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'Project/Flickr 8k Dataset'\n",
    "WORKING_DIR = 'Project'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695a062-6854-4e11-ab8c-91df096b3eb2",
   "metadata": {},
   "source": [
    "## Load VGG16 model without top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4bbe5a-510c-4fea-8242-7d1f2e49c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load vgg16 model\n",
    "vgg_model = VGG16(weights='imagenet')  \n",
    "# restructure the model\n",
    "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n",
    "# summarize\n",
    "print(vgg_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66a2d2-0fc8-4094-a3dc-d1150e6ce55e",
   "metadata": {},
   "source": [
    "## Extract the image features and load the data for preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70cd66f0-5783-4c6e-81c0-6aecb40407af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(): \n",
    "    # Extract features from all images\n",
    "    features = {}\n",
    "    directory = os.path.join(BASE_DIR, 'Images')\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\" Error: Images directory not found at {directory}\")\n",
    "        return None\n",
    "    \n",
    "    image_files = [f for f in os.listdir(directory) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    for img_name in tqdm(image_files, desc=\"Extracting features\"):\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(directory, img_name)\n",
    "            image = load_img(img_path, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            \n",
    "            # Extract features\n",
    "            feature = vgg_model.predict(image, verbose=0)\n",
    "            \n",
    "            # Store feature with image ID (without extension)\n",
    "            image_id = img_name.split('.')[0]\n",
    "            features[image_id] = feature\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_name}: {e}\")\n",
    "    \n",
    "    print(f\"Feature extraction completed. Extracted features for {len(features)} images\")\n",
    "    \n",
    "    # Save features to pickle file\n",
    "    pickle_path = os.path.join(WORKING_DIR, 'features.pkl')\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "    print(f\"Features saved to {pickle_path}\")\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afaf73-f4e8-49e4-871d-8f30d475cb13",
   "metadata": {},
   "source": [
    "## Load features from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b5849e-d95a-4d21-b1c0-4f0401de6c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-extracted features found. Starting extraction...\n",
      "Found 8091 images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936474db297b4f85ad30c392c5779b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed. Extracted features for 8091 images\n",
      "Features saved to Project\\features.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_image_features():\n",
    "    \"\"\"Load pre-extracted image features\"\"\"\n",
    "    pickle_path = os.path.join(WORKING_DIR, 'features.pkl')\n",
    "    \n",
    "    if os.path.exists(pickle_path):\n",
    "        print(\"Loading pre-extracted features...\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        print(f\" Loaded features for {len(features)} images\")\n",
    "        return features\n",
    "    else:\n",
    "        print(\"No pre-extracted features found. Starting extraction...\")\n",
    "        return extract_image_features()\n",
    "\n",
    "# Extract or load features\n",
    "features = load_image_features()\n",
    "\n",
    "if features is None:\n",
    "    print(\" Failed to extract features. Please check your image directory.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b231d-11dc-4c11-9c34-21ff6991a318",
   "metadata": {},
   "source": [
    "## LOAD AND PROCESS CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac42394d-bd6c-48b5-9c8b-af7eef077428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7209d50a6547c19e183538f056d6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing captions:   0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded captions for 8091 images\n",
      "Total captions: 40455\n"
     ]
    }
   ],
   "source": [
    "def load_captions():\n",
    "    \"\"\"Load captions from the text file\"\"\"\n",
    "    captions_path = os.path.join(BASE_DIR, 'captions.txt')\n",
    "    \n",
    "    if not os.path.exists(captions_path):\n",
    "        print(f\" Error: Captions file not found at {captions_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Loading captions...\")\n",
    "    with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # Skip header\n",
    "        captions_doc = f.read()\n",
    "    \n",
    "    # Create mapping of image_id to captions\n",
    "    mapping = {}\n",
    "    for line in tqdm(captions_doc.split('\\n'), desc=\"Processing captions\"):\n",
    "        if len(line.strip()) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Split by comma - first part is image_id, rest is caption\n",
    "        tokens = line.split(',')\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        \n",
    "        image_id = tokens[0].split('.')[0]  # Remove file extension\n",
    "        caption = ','.join(tokens[1:])  # Rejoin in case caption contains commas\n",
    "        \n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = []\n",
    "        \n",
    "        mapping[image_id].append(caption)\n",
    "    \n",
    "    print(f\"Loaded captions for {len(mapping)} images\")\n",
    "    print(f\"Total captions: {sum(len(caps) for caps in mapping.values())}\")\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Load captions\n",
    "mapping = load_captions()\n",
    "\n",
    "if mapping is None:\n",
    "    print(\"Failed to load captions. Please check your captions file.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a480e91-5359-499f-857b-e7a2bd38792e",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d6748e-88a2-405f-94c6-3b5ab322ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning captions...\n",
      " Cleaned 40455 captions\n",
      "\n",
      "Sample cleaned captions:\n",
      "  1. startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
      "  2. startseq girl going into wooden building endseq\n",
      "  3. startseq little girl climbing into wooden playhouse endseq\n"
     ]
    }
   ],
   "source": [
    "def clean_captions(mapping):\n",
    "    \"\"\"Clean and preprocess caption text\"\"\"\n",
    "    print(\"Cleaning captions...\")\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            \n",
    "            # Remove punctuation and special characters\n",
    "            caption = re.sub(r'[^a-zA-Z\\s]', '', caption)\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "            \n",
    "            # Remove very short words (length < 2)\n",
    "            words = [word for word in caption.split() if len(word) >= 2]\n",
    "            \n",
    "            # Add start and end tokens\n",
    "            caption = 'startseq ' + ' '.join(words) + ' endseq'\n",
    "            \n",
    "            captions[i] = caption\n",
    "            cleaned_count += 1\n",
    "    \n",
    "    print(f\" Cleaned {cleaned_count} captions\")\n",
    "    \n",
    "    # Show some examples\n",
    "    sample_key = list(mapping.keys())[0]\n",
    "    print(\"\\nSample cleaned captions:\")\n",
    "    for i, caption in enumerate(mapping[sample_key][:3]):\n",
    "        print(f\"  {i+1}. {caption}\")\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Clean captions\n",
    "mapping = clean_captions(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1803aa-2134-4d55-a90b-119261a5cf46",
   "metadata": {},
   "source": [
    "## TOKENIZATION AND VOCABULARY CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97520c0c-c05a-4c68-968c-029488fe8d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenizer...\n",
      "Total captions for tokenization: 40455\n",
      "✓ Vocabulary size: 8768\n",
      "✓ Maximum sequence length: 34\n",
      "\n",
      "Most common words:\n",
      "  'startseq': 40455\n",
      "  'endseq': 40455\n",
      "  'in': 18974\n",
      "  'the': 18418\n",
      "  'on': 10743\n",
      "  'is': 9345\n",
      "  'and': 8851\n",
      "  'dog': 8136\n",
      "  'with': 7765\n",
      "  'man': 7265\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer(mapping):\n",
    "    \"\"\"Create tokenizer from all captions\"\"\"\n",
    "    print(\"Creating tokenizer...\")\n",
    "    \n",
    "    # Collect all captions\n",
    "    all_captions = []\n",
    "    for key in mapping:\n",
    "        for caption in mapping[key]:\n",
    "            all_captions.append(caption)\n",
    "    \n",
    "    print(f\"Total captions for tokenization: {len(all_captions)}\")\n",
    "    \n",
    "    # Create and fit tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Calculate max sequence length\n",
    "    max_length = max(len(caption.split()) for caption in all_captions)\n",
    "    \n",
    "    print(f\"✓ Vocabulary size: {vocab_size}\")\n",
    "    print(f\"✓ Maximum sequence length: {max_length}\")\n",
    "    \n",
    "    # Show most common words\n",
    "    word_counts = [(word, tokenizer.word_counts[word]) for word in tokenizer.word_index]\n",
    "    word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nMost common words:\")\n",
    "    for word, count in word_counts[:10]:\n",
    "        print(f\"  '{word}': {count}\")\n",
    "    \n",
    "    return tokenizer, vocab_size, max_length, all_captions\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer, vocab_size, max_length, all_captions = create_tokenizer(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09776c-a3ff-42a4-ac5f-35e8fcd36ad6",
   "metadata": {},
   "source": [
    "## TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a162a3e5-caad-4786-9a8e-fc424645f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and test sets...\n",
      "Images with both captions and features: 8091\n",
      " Training images: 7281\n",
      " Test images: 810\n"
     ]
    }
   ],
   "source": [
    "def split_data(mapping, features, test_size=0.1):\n",
    "    \"\"\"Split data into train and test sets\"\"\"\n",
    "    print(\"Splitting data into train and test sets...\")\n",
    "    \n",
    "    # Get image IDs that have both features and captions\n",
    "    valid_image_ids = [img_id for img_id in mapping.keys() if img_id in features]\n",
    "    \n",
    "    print(f\"Images with both captions and features: {len(valid_image_ids)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        valid_image_ids, \n",
    "        test_size=test_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\" Training images: {len(train_ids)}\")\n",
    "    print(f\" Test images: {len(test_ids)}\")\n",
    "    \n",
    "    return train_ids, test_ids\n",
    "\n",
    "# Split data\n",
    "train_ids, test_ids = split_data(mapping, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912d16f-cbdb-412d-91a2-c3e1f6f4606a",
   "metadata": {},
   "source": [
    "## DATA GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a0a8fa-abe1-43dc-b445-da8dfba463c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    \"\"\"Create data generator for training\"\"\"\n",
    "    \n",
    "    def data_generator():\n",
    "        while True:\n",
    "            X1, X2, y = [], [], []\n",
    "            \n",
    "            for key in data_keys:\n",
    "                if key not in features:\n",
    "                    continue\n",
    "                \n",
    "                captions = mapping[key]\n",
    "                for caption in captions:\n",
    "                    # Encode caption to sequence\n",
    "                    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    \n",
    "                    # Only process sequences with reasonable length\n",
    "                    if len(seq) < 3:  # Too short\n",
    "                        continue\n",
    "                    \n",
    "                    # Pad sequence\n",
    "                    seq = pad_sequences([seq], maxlen=max_length, padding='post')[0]\n",
    "                    \n",
    "                    # Create input-output pairs\n",
    "                    input_seq = seq[:-1]  # All tokens except last\n",
    "                    target_seq = seq[1:]  # All tokens except first\n",
    "                    \n",
    "                    # Add to batch\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(input_seq)\n",
    "                    y.append(target_seq)\n",
    "                    \n",
    "                    # Yield batch when ready\n",
    "                    if len(X1) == batch_size:\n",
    "                        yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                        X1, X2, y = [], [], []\n",
    "            \n",
    "            # Yield remaining data\n",
    "            if len(X1) > 0:\n",
    "                yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "    \n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101707f-71f1-4482-b628-4454568a9137",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cbcea1-81d5-4a8c-a737-66b1cce78416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "EMBED_DIM = 256\n",
    "LSTM_UNITS = 512\n",
    "DROPOUT_RATE = 0.3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9635790-6bb9-4b9b-9ee4-0fa30df1479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model architecture...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      " Model built successfully\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['image[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " text (InputLayer)              [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 33, 256)      2244608     ['text[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 256)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 33, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (None, 33, 256)      0           ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 33, 512)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'tf.tile[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 33, 512)      2099200     ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 33, 512)      2099200     ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 33, 512)      262656      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 33, 512)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 33, 256)      131328      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 33, 256)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 33, 8768)     2253376     ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,204,992\n",
      "Trainable params: 10,204,992\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      " Model architecture saved to caption_model_architecture.png\n"
     ]
    }
   ],
   "source": [
    "def build_caption_model(vocab_size, max_length, embed_dim=EMBED_DIM, lstm_units=LSTM_UNITS):\n",
    "    \"\"\"Build the caption generation model\"\"\"\n",
    "    print(\"Building model architecture...\")\n",
    "    \n",
    "    # Image input\n",
    "    image_input = Input(shape=(4096,), name='image')\n",
    "    image_dense = Dense(embed_dim, activation='relu')(image_input)\n",
    "    image_dense = Dropout(DROPOUT_RATE)(image_dense)\n",
    "    image_dense = Dense(embed_dim, activation='relu')(image_dense)  # Additional layer\n",
    "    \n",
    "    # Text input\n",
    "    text_input = Input(shape=(max_length-1,), name='text')\n",
    "    text_embed = Embedding(vocab_size, embed_dim, mask_zero=True)(text_input)\n",
    "    text_embed = Dropout(DROPOUT_RATE)(text_embed)\n",
    "    \n",
    "    # Repeat image features for each time step\n",
    "    image_seq = tf.expand_dims(image_dense, 1)\n",
    "    image_seq = tf.tile(image_seq, [1, max_length-1, 1])\n",
    "    \n",
    "    # Combine image and text features\n",
    "    combined = tf.concat([text_embed, image_seq], axis=-1)\n",
    "    \n",
    "    # LSTM layers\n",
    "    lstm1 = LSTM(lstm_units, return_sequences=True, dropout=DROPOUT_RATE, \n",
    "                 recurrent_dropout=DROPOUT_RATE)(combined)\n",
    "    lstm2 = LSTM(lstm_units, return_sequences=True, dropout=DROPOUT_RATE, \n",
    "                 recurrent_dropout=DROPOUT_RATE)(lstm1)\n",
    "    \n",
    "    # Dense layers\n",
    "    dense = Dense(512, activation='relu')(lstm2)\n",
    "    dense = Dropout(0.4)(dense)\n",
    "    dense = Dense(256, activation='relu')(dense)\n",
    "    dense = Dropout(0.4)(dense)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(vocab_size, activation='softmax')(dense)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\" Model built successfully\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_caption_model(vocab_size, max_length)\n",
    "\n",
    "# Plot model architecture\n",
    "try:\n",
    "    plot_model(model, to_file='caption_model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "    print(\" Model architecture saved to caption_model_architecture.png\")\n",
    "except:\n",
    "    print(\" Could not save model architecture plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa4207-cb96-4a18-ae03-68d49e833632",
   "metadata": {},
   "source": [
    "## TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd612320-2246-4a67-81f1-8e0b566117df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training...\n",
      "Training samples: 36405\n",
      "Validation samples: 4050\n",
      "Steps per epoch - Train: 1137, Validation: 126\n"
     ]
    }
   ],
   "source": [
    "def setup_training():\n",
    "    \"\"\"Setup training parameters and callbacks\"\"\"\n",
    "    print(\"Setting up training...\")\n",
    "    \n",
    "    # Calculate steps per epoch\n",
    "    train_samples = sum(len(mapping[key]) for key in train_ids if key in mapping)\n",
    "    val_samples = sum(len(mapping[key]) for key in test_ids if key in mapping)\n",
    "    \n",
    "    train_steps = max(1, train_samples // BATCH_SIZE)\n",
    "    val_steps = max(1, val_samples // BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Validation samples: {val_samples}\")\n",
    "    print(f\"Steps per epoch - Train: {train_steps}, Validation: {val_steps}\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_caption_model.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.8,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create generators\n",
    "    train_gen_func = create_data_generator(\n",
    "        train_ids, mapping, features, tokenizer, max_length, vocab_size, BATCH_SIZE\n",
    "    )\n",
    "    val_gen_func = create_data_generator(\n",
    "        test_ids, mapping, features, tokenizer, max_length, vocab_size, BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    train_generator = train_gen_func()\n",
    "    val_generator = val_gen_func()\n",
    "    \n",
    "    return train_generator, val_generator, train_steps, val_steps, callbacks\n",
    "\n",
    "# Setup training\n",
    "train_generator, val_generator, train_steps, val_steps, callbacks = setup_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ba6e6-ad2d-441a-ba20-2524c98cfdb2",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a62ea88-fe6d-48c5-84ed-dc021899d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to start training!\n",
      "Uncomment the next line to begin training:\n",
      "# history = train_model()\n",
      "\n",
      "============================================================\n",
      "STARTING MODEL TRAINING\n",
      "============================================================\n",
      "Epochs: 25\n",
      "Batch size: 32\n",
      "Learning rate: 0.0005\n",
      "============================================================\n",
      "Epoch 1/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.7457 - accuracy: 0.7314\n",
      "Epoch 1: val_loss improved from inf to 1.45058, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 491s 422ms/step - loss: 1.7457 - accuracy: 0.7314 - val_loss: 1.4506 - val_accuracy: 0.7561 - lr: 5.0000e-04\n",
      "Epoch 2/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.4353 - accuracy: 0.7553\n",
      "Epoch 2: val_loss improved from 1.45058 to 1.34676, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 471s 414ms/step - loss: 1.4353 - accuracy: 0.7553 - val_loss: 1.3468 - val_accuracy: 0.7640 - lr: 5.0000e-04\n",
      "Epoch 3/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.7639\n",
      "Epoch 3: val_loss improved from 1.34676 to 1.27622, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 459s 404ms/step - loss: 1.3393 - accuracy: 0.7639 - val_loss: 1.2762 - val_accuracy: 0.7722 - lr: 5.0000e-04\n",
      "Epoch 4/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.2684 - accuracy: 0.7699\n",
      "Epoch 4: val_loss improved from 1.27622 to 1.23011, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 460s 404ms/step - loss: 1.2684 - accuracy: 0.7699 - val_loss: 1.2301 - val_accuracy: 0.7764 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.2155 - accuracy: 0.7740\n",
      "Epoch 5: val_loss improved from 1.23011 to 1.19651, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 464s 408ms/step - loss: 1.2155 - accuracy: 0.7740 - val_loss: 1.1965 - val_accuracy: 0.7797 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.1735 - accuracy: 0.7774\n",
      "Epoch 6: val_loss improved from 1.19651 to 1.17029, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 459s 404ms/step - loss: 1.1735 - accuracy: 0.7774 - val_loss: 1.1703 - val_accuracy: 0.7829 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.1390 - accuracy: 0.7800\n",
      "Epoch 7: val_loss improved from 1.17029 to 1.15341, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 457s 402ms/step - loss: 1.1390 - accuracy: 0.7800 - val_loss: 1.1534 - val_accuracy: 0.7846 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.1108 - accuracy: 0.7822\n",
      "Epoch 8: val_loss improved from 1.15341 to 1.14223, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 478s 421ms/step - loss: 1.1108 - accuracy: 0.7822 - val_loss: 1.1422 - val_accuracy: 0.7855 - lr: 5.0000e-04\n",
      "Epoch 9/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0869 - accuracy: 0.7841\n",
      "Epoch 9: val_loss improved from 1.14223 to 1.13297, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 482s 424ms/step - loss: 1.0869 - accuracy: 0.7841 - val_loss: 1.1330 - val_accuracy: 0.7869 - lr: 5.0000e-04\n",
      "Epoch 10/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0660 - accuracy: 0.7855\n",
      "Epoch 10: val_loss improved from 1.13297 to 1.12855, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 496s 436ms/step - loss: 1.0660 - accuracy: 0.7855 - val_loss: 1.1285 - val_accuracy: 0.7887 - lr: 5.0000e-04\n",
      "Epoch 11/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.7873\n",
      "Epoch 11: val_loss improved from 1.12855 to 1.12384, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 466s 410ms/step - loss: 1.0467 - accuracy: 0.7873 - val_loss: 1.1238 - val_accuracy: 0.7896 - lr: 5.0000e-04\n",
      "Epoch 12/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0302 - accuracy: 0.7886\n",
      "Epoch 12: val_loss did not improve from 1.12384\n",
      "1137/1137 [==============================] - 476s 418ms/step - loss: 1.0302 - accuracy: 0.7886 - val_loss: 1.1260 - val_accuracy: 0.7899 - lr: 5.0000e-04\n",
      "Epoch 13/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0163 - accuracy: 0.7897\n",
      "Epoch 13: val_loss improved from 1.12384 to 1.11897, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 498s 438ms/step - loss: 1.0163 - accuracy: 0.7897 - val_loss: 1.1190 - val_accuracy: 0.7914 - lr: 5.0000e-04\n",
      "Epoch 14/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 1.0030 - accuracy: 0.7909\n",
      "Epoch 14: val_loss did not improve from 1.11897\n",
      "1137/1137 [==============================] - 478s 421ms/step - loss: 1.0030 - accuracy: 0.7909 - val_loss: 1.1195 - val_accuracy: 0.7919 - lr: 5.0000e-04\n",
      "Epoch 15/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9917 - accuracy: 0.7917\n",
      "Epoch 15: val_loss improved from 1.11897 to 1.11774, saving model to best_caption_model.h5\n",
      "1137/1137 [==============================] - 464s 408ms/step - loss: 0.9917 - accuracy: 0.7917 - val_loss: 1.1177 - val_accuracy: 0.7929 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9800 - accuracy: 0.7926\n",
      "Epoch 16: val_loss did not improve from 1.11774\n",
      "1137/1137 [==============================] - 473s 416ms/step - loss: 0.9800 - accuracy: 0.7926 - val_loss: 1.1206 - val_accuracy: 0.7934 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9703 - accuracy: 0.7935\n",
      "Epoch 17: val_loss did not improve from 1.11774\n",
      "1137/1137 [==============================] - 471s 414ms/step - loss: 0.9703 - accuracy: 0.7935 - val_loss: 1.1249 - val_accuracy: 0.7934 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9617 - accuracy: 0.7944\n",
      "Epoch 18: val_loss did not improve from 1.11774\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "1137/1137 [==============================] - 466s 410ms/step - loss: 0.9617 - accuracy: 0.7944 - val_loss: 1.1225 - val_accuracy: 0.7941 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9479 - accuracy: 0.7958\n",
      "Epoch 19: val_loss did not improve from 1.11774\n",
      "1137/1137 [==============================] - 464s 408ms/step - loss: 0.9479 - accuracy: 0.7958 - val_loss: 1.1245 - val_accuracy: 0.7948 - lr: 4.0000e-04\n",
      "Epoch 20/25\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 0.9398 - accuracy: 0.7967Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.11774\n",
      "1137/1137 [==============================] - 465s 409ms/step - loss: 0.9398 - accuracy: 0.7967 - val_loss: 1.1292 - val_accuracy: 0.7946 - lr: 4.0000e-04\n",
      "Epoch 20: early stopping\n",
      "\n",
      " Training completed!\n",
      " Final model saved\n",
      " Tokenizer saved\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Train the caption generation model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Training completed!\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save('final_caption_model.h5')\n",
    "    print(\" Final model saved\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    with open('tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\" Tokenizer saved\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Start training\n",
    "print(\"Ready to start training!\")\n",
    "print(\"Uncomment the next line to begin training:\")\n",
    "print(\"# history = train_model()\")\n",
    "\n",
    "# start training:\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51676643-46ae-408d-ad2f-deb27568688b",
   "metadata": {},
   "source": [
    "## INFERENCE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3accc2-d4db-4088-ab7e-bbebe890a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, image_feature, max_length, method='greedy'):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    \n",
    "    if method == 'greedy':\n",
    "        return generate_caption_greedy(model, tokenizer, image_feature, max_length)\n",
    "    elif method == 'beam_search':\n",
    "        return generate_caption_beam_search(model, tokenizer, image_feature, max_length)\n",
    "    else:\n",
    "        return generate_caption_greedy(model, tokenizer, image_feature, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756cd921-863e-4fcd-862a-224a035fb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_greedy(model, tokenizer, image_feature, max_length):\n",
    "    \"\"\"Generate caption using greedy search\"\"\"\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # Encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length-1, padding='post')\n",
    "        \n",
    "        # Predict next word\n",
    "        y_pred = model.predict([image_feature.reshape(1, -1), sequence], verbose=0)\n",
    "        \n",
    "        # Get word with highest probability\n",
    "        y_pred = np.argmax(y_pred[0, len(tokenizer.texts_to_sequences([in_text])[0])-1, :])\n",
    "        \n",
    "        # Convert index to word\n",
    "        word = None\n",
    "        for word_text, index in tokenizer.word_index.items():\n",
    "            if index == y_pred:\n",
    "                word = word_text\n",
    "                break\n",
    "        \n",
    "        # Stop if no word found or end sequence\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        \n",
    "        # Add word to sequence\n",
    "        in_text += ' ' + word\n",
    "    \n",
    "    # Remove start sequence\n",
    "    caption = in_text.replace('startseq ', '')\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc75dcdf-422c-482a-9368-7196ade1805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_beam_search(model, tokenizer, image_feature, max_length, beam_width=3):\n",
    "    \"\"\"Generate caption using beam search\"\"\"\n",
    "    # Initialize beam\n",
    "    sequences = [(['startseq'], 0.0)]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in sequences:\n",
    "            if seq[-1] == 'endseq':\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Encode sequence\n",
    "            text = ' '.join(seq)\n",
    "            encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "            padded = pad_sequences([encoded], maxlen=max_length-1, padding='post')\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = model.predict([image_feature.reshape(1, -1), padded], verbose=0)\n",
    "            \n",
    "            # Get top predictions\n",
    "            position = min(len(encoded)-1, max_length-2)\n",
    "            top_indices = np.argsort(preds[0, position, :])[-beam_width:]\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                # Find word for index\n",
    "                word = None\n",
    "                for w, i in tokenizer.word_index.items():\n",
    "                    if i == idx:\n",
    "                        word = w\n",
    "                        break\n",
    "                \n",
    "                if word:\n",
    "                    new_seq = seq + [word]\n",
    "                    new_score = score + np.log(preds[0, position, idx] + 1e-8)\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top sequences\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        # Check if all ended\n",
    "        if all(seq[-1] == 'endseq' for seq, _ in sequences):\n",
    "            break\n",
    "    \n",
    "    # Return best sequence\n",
    "    best_seq = sequences[0][0]\n",
    "    caption = ' '.join(best_seq[1:]).replace(' endseq', '')\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13cb5f3e-6c5c-4188-983f-edaaca324fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption_for_image(image_path, model, tokenizer, max_length):\n",
    "    \"\"\"Complete pipeline for new image\"\"\"\n",
    "    # Load VGG16 model\n",
    "    vgg_model = VGG16(weights='imagenet')\n",
    "    vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n",
    "    \n",
    "    # Extract features\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = vgg_model.predict(image, verbose=0)\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = generate_caption(model, tokenizer, feature, max_length)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb22f0-028c-4be8-b3ec-a333c2855740",
   "metadata": {},
   "source": [
    "## EVALUATION AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9525273a-156f-4d31-a288-2205e8cb2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_ids, mapping, features, tokenizer, max_length, num_samples=5):\n",
    "    \"\"\"Evaluate model on test samples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, image_id in enumerate(test_ids[:num_samples]):\n",
    "        if image_id not in features or image_id not in mapping:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Test {i+1}: Image {image_id} ---\")\n",
    "        \n",
    "        # Actual captions\n",
    "        actual_captions = mapping[image_id]\n",
    "        print(\"Actual captions:\")\n",
    "        for j, caption in enumerate(actual_captions):\n",
    "            print(f\"  {j+1}. {caption}\")\n",
    "        \n",
    "        # Generated caption\n",
    "        try:\n",
    "            generated = generate_caption(model, tokenizer, features[image_id], max_length)\n",
    "            print(f\"\\nGenerated caption: {generated}\")\n",
    "            \n",
    "            # Also try beam search\n",
    "            generated_beam = generate_caption(model, tokenizer, features[image_id], max_length, method='beam_search')\n",
    "            print(f\"Generated (beam search): {generated_beam}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "885932ef-2454-44b4-afc6-9b42dbfe610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    if history is None:\n",
    "        print(\"No training history available\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abd7a3-eccf-4b07-a025-0f4f809518bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
